"""
Date: Thu, Sat, 18 Feb, 2017
Last updated: Sat, 27 Mar, 2017
Author: Samuel Barham
Organization: University of Maryland, College Park (PhD student)

Project: RAE/SeRPE implementation
Component: Planning-DSL Interpreter

Description:
"Lexer" is an abbreviated form of "lexical analyzer" -- also known as a
tokenizer. Lexing, or tokenization, is the process by which a stream of raw
character-data is transformed into a stream of lexemes (tokens) -- each of which
is a sequence of characters from some language L, over some alphabet S. A lexer
is typically implemented as a table-driven finite state automaton -- and those
generated by the popular UNIX tool, lex, are no exception. Lex, in its turn, is
an abbreviation of "lexical analyzer generator," and it does just what you would
expect it to -- it generates lexical analyzers (lexers), based on rules supplied
by you (the user) in the proper format. To put it simply, these rules define the
set of lexemes (tokens) that compose the language -- and, additionally, often
imposes a type system on on that language, perhaps classifying the lexeme
"142,000" as an INT (for example) or the lexeme "some_var" as an ID. This token
stream is then often used by a parser -- which identifies syntactic relations
between lexemes in the token stream, aggregating the tokens into larger struc-
tures.

In particular, this file, dom_lexer.py contains the lexer that lexes the
planning domain-description files we've designed for our RAE/SeRPE system. This
lexer differs from most lexers, however, in that the resulting token stream is
merely discarded, and not used. Rather, this lexer builds during the lexing pro-
cess a symbol table representing the planning domain described in the .dom
file -- a table table that contains, for example, the set of valid objects; the
set of rigid relationships and their values (these are essentially environmental
constants); and the set of state variables, with their ranges.

PLY is a popular Python-implementation of the UNIX lex/yacc tool, and this
file defines the rules by which PLY will create the lexer for our method-
definition language (i.e., the domain-specific language in which we will define
the methods in a given planning domain).

More information on the PLY module and how it works can be found at the
following URL: http://www.dabeaz.com/ply/ply.html#ply_nn1
"""


"""
TOKEN LIST
"""


"""
FLAGS
"""

objects_flag = False    # we're in the 'objects' section of the file
rigid_flag = False      # we're in the 'rigid relations' section of the file
state_flag = False      # we're in the 'state-variable ranges' section of the file
s0_flag = False         # we're in the 'initial state' section of the file
goal_flag = False       # we're in the 'goal' section of the file

def all_false():
    objects_flag = False
    rigid_rels_flag = False
    state_vars_flag = False
    s0_flag = False
    goal_flag = Flase


"""
TOKENS
"""




"""
RULES
"""

def t_objects(t):
    r'(O|o)bjects:'
    all_false()
    objects_flag = True
    pass

def t_rigid_rels(t):
    r'(R|r)igid\W+(R|r)elations:'
    all_false()
    objects_flag = True
    pass

def t_state_vars(t):
    r'(S|s)tate(-|\W+)(V|v)ariable.*:'
    all_false()
    objects_flag = True
    pass

def t_s0(t):
    r'(I|i)nitial.*:'
    all_false()
    objects_flag = True
    pass

def t_goal(t):
    r'(G|g)oal.*:'
    all_false()
    objects_flag = True
    pass


"""
HELPER FUNCTIONS
"""
# This allows us to track line numbers.
# Since it's not a rule -- but rather a kind of helper -- we do not capitalize
# it's identifier.
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)  # counts how many new lines and adds that
                                    # number to current lineno

# This helper rule defines what we do when we encounter a lexing error
def t_error(t):
    print("Illegal character '%s'" % t.value[0])
    t.lexer.skip(1)

def find_column(input,token):
    """
    This helper function computes the column at which the current token begins.
    This is super useful for error-handling. Here 'input' is the input text
    string and 'token' is a token instance.
    """
    last_cr = input.rfind('\n',0,token.lexpos)
    if last_cr < 0:
        last_cr = 0
        column = (token.lexpos - last_cr) + 1
    return column

'''
EXAMPLE FILE:

Objects:
misc = {nil}
dock = {d1, d2}
robot = {r1, r2}
cargo = {c1, c2, c3}
pile = {p1, p2, p3, p4}

Rigid relations:
adjacent = {(d1, d2)}
on-dock = {(p1, d1), (p2, d1), (p3, d2), (p4, d2)}

State-variable ranges:
on-robot(_) = {c1, c2, c3, nil}
loc(_) = {d1, d2}
in-pile(_) = {p1, p2, p3, p4}

Initial state:
loc(r1) = d1
loc(r2) = d2
in-pile(c1) = p1
in-pile(c2) = p2
in-pile(c3) = p3
on-robot(r1) = nil
on-robot(r2) = nil

Goal:
in-pile(c3) = p4
'''
